}
########################
getFolderAccuracy("../results/imputationTests/testFAIR_15/","imputeTest_FAIR_15_*")
library(reshape2)
library(ggplot2)
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/")
load("../data/EA_imputed/preImputed.Rdat")
soc.id = eadx[,1]
# take out socid
eadx = eadx[,-1]
getAccuracy = function(f){
#print(f)
load(f)
if(nrow(eadx2)==nrow(eadx)){
test.arrInd = test.arrInd[test.arrInd[,1]!=0,]
accuracy = sum(eadx2[test.arrInd] == eadx[test.arrInd])/nrow(test.arrInd)
# baseline
baseline = replicate(100,getRandomBaseline(test.arrInd,eadx,eadx2))
z = (accuracy - mean(baseline))/sd(baseline)
baseline2 = replicate(100,getRandomBaseline_totallyRandom(test.arrInd,eadx,eadx2))
z2 = (accuracy - mean(baseline2))/sd(baseline2)
return(c(accuracy=accuracy, baseline=mean(baseline), z=z,
baseline2 = mean(baseline2), z2 = z2))
} else{
print("Row numbers don't match")
}
}
getRandomBaseline = function(test.arrInd, eadx, eadx2){
# imputation by random sampling
randomBaseline =
sapply(test.arrInd[,2], function(X){
sx = eadx2[,X]
sx = sx[!is.na(sx)]
sample(sx,1)
})
sum(randomBaseline == eadx[test.arrInd])/nrow(test.arrInd)
}
getRandomBaseline_totallyRandom = function(test.arrInd, eadx, eadx2){
# imputation by random sampling of set (no frequencies)
randomBaseline =
sapply(test.arrInd[,2], function(X){
sx = eadx2[,X]
sx = sx[!is.na(sx)]
sample(unique(sx),1)
})
sum(randomBaseline == eadx[test.arrInd])/nrow(test.arrInd)
}
getFolderAccuracy = function(folder,prefix){
accuracy.orig = sapply(
paste0(folder,
list.files(folder,prefix)),
getAccuracy)
accuracy.orig = as.data.frame(t(accuracy.orig))
return(c(mean(accuracy.orig$accuracy,na.rm=T),
(mean(accuracy.orig$baseline,na.rm=T)),
(mean(accuracy.orig$z,na.rm=T)),
(mean(accuracy.orig$baseline2,na.rm=T)),
(mean(accuracy.orig$z2,na.rm=T))))
}
getFolderAccuracy("../results/imputationTests/testFAIR_15/","imputeTest_FAIR_15_*")
---
title: "Cultural distances: controlling for history"
output: pdf_document
---
```{r echo=F, eval=F}
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/")
```
# Introduction
We compare cultural distances between socieites with linguistic distances between societies, controlling for shared history in two ways.
The first test uses mixed effects modelling.  The pairing of the langauge family of each language (according to Glottolog) is used as a random effect.  That means that the model can capture the likelihood that two languages from the Indo-European langauge family will be more similar to each other than two languages from different langauge families.  The same is done with geographic area according to Autotyp.
The second test controls for history using distances from a phylogenetic tree.  The tree comes from Bouckaert et al. (2012).  Patristic distances between languages are used as a measure of historical distance between societies in a Mantel test.  Note that the Mantel test assumes a strict distance metric, which is not necessarily the case with this data, but there are few other ways to deal with continuous pairwise distances.
# Load libraries
```{r warning=F, message=F}
library(ape)
library(ecodist)
library(lme4)
library(sjPlot)
library(ggplot2)
library(igraph)
library(lattice)
```
# All domains
## Load data
Read the cultural distances:
```{r}
cult = read.csv("../results/EA_distances/CulturalDistances_Long.csv", stringsAsFactors = F)
names(cult) = c("l1","l2","cult.dist")
cultLangs = unique(c(cult$Var1,cult$Var2))
```
Add language family:
```{r}
l = read.csv("../data/FAIR_langauges_glotto_xdid.csv", stringsAsFactors = F)
g = read.csv("../data/glottolog-languoid.csv/languoid.csv", stringsAsFactors = F)
l$family = g[match(l$glotto,g$id),]$family_pk
l$family = g[match(l$family,g$pk),]$name
```
Read the semantic distances
```{r}
ling = read.csv("../data/FAIR/semantic_distances_FAIR.csv", stringsAsFactors = F)
```
Combine the lingusitic and cultural distances
```{r}
cult$l1.iso2 = l[match(cult$l1,l$Language2),]$iso2
cult$l2.iso2 = l[match(cult$l2,l$Language2),]$iso2
fairisos = unique(c(ling$l1,ling$l2))
cultisos = unique(c(cult$l1.iso2, cult$l2.iso2))
cult = cult[(cult$l1.iso2 %in% fairisos) & (cult$l2.iso2 %in% fairisos),]
ling = ling[(ling$l1 %in% cultisos) & (ling$l2 %in% cultisos),]
matches = sapply(1:nrow(ling), function(i){
which(cult$l1.iso2==ling$l1[i] & cult$l2.iso2==ling$l2[i])
})
ling$cult.dist = cult[matches,]$cult.dist
ling$cult.dist.center = scale(ling$cult.dist)
cdc.s = attr(ling$cult.dist.center,"scaled:scale")
cdc.c = attr(ling$cult.dist.center,"scaled:center")
ling$cult.dist.center = as.numeric(ling$cult.dist.center)
ling$family1 = l[match(ling$l1, l$iso2),]$family
ling$area1 = l[match(ling$l1, l$iso2),]$autotyp.area
ling$family2 = l[match(ling$l2, l$iso2),]$family
ling$area2 = l[match(ling$l2, l$iso2),]$autotyp.area
ling$family.group = paste(sort(c(ling$family1,ling$family2)))
ling$area.group = paste(sort(c(ling$area1,ling$area2)))
ling$rho.center = scale(ling$rho)
```
Each observation is now assocaited with a langauge family pair:
```{r}
head(ling[,c("l1","l2","rho",'family.group')])
```
And the same is true for area:
```{r}
tail(ling[,c("l1","l2","rho",'area.group')])
```
## LMER models
Mixed effects model, predicting linguistic distances from cultural distances, with random intercept for family and area and random slope for cultural distance for family and area.
We compare a null model to a model with a fixed effect for cultural distance, with random intercepts for family and area, and random slopes for cultural distance by both.
```{r}
m0 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0,m1)
```
Cultural distance is significantly correlated with linguistic distance.  Here are the model estimates:
```{r}
summary(m1)
```
Plot the estimates, rescaling the variables back to the original units:
```{r}
gx = sjp.lmer(m1,'pred','cult.dist.center', prnt.plot = F)
gx$plot$data$y = gx$plot$data$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
gx$plot$data$resp.y = gx$plot$data$resp.y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
gx$plot$data$x = gx$plot$data$x *
cdc.s +cdc.c
gx$plot + coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
```
Plot the random effects.  There seems to be more variation by area:
```{r}
sjp.lmer(m1,'re', sort.est = "cult.dist.center")
px = sjp.lmer(m1,'rs.ri', prnt.plot = F)
dx = px$plot[[1]]$data
dx$x = dx$x * cdc.s + cdc.c
dx$y = dx$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
ggplot(dx,aes(x,y)) +
geom_point(data=ling,
mapping=aes(x=as.numeric(cult.dist),
y=as.numeric(rho))) +
geom_line(mapping = aes(colour=grp)) +
xlab("Cultural distance")+
ylab("Linguistic distance") +
ggtitle("Area pair random effects") +
coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
dx = px$plot[[2]]$data
dx$x = dx$x * cdc.s + cdc.c
dx$y = dx$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
ggplot(dx,aes(x,y)) +
geom_point(data=ling,
mapping=aes(x=as.numeric(cult.dist),
y=as.numeric(rho))) +
geom_line(mapping = aes(colour=grp)) +
xlab("Cultural distance")+
ylab("Linguistic distance") +
ggtitle("Family pair random effects") +
coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
```
\newpage
# Tests within domains
Load distances for specific domains and match up to language family and area:
```{r}
ling.dom = read.csv(
"../results/EA_distances/All_Domains_with_ling.csv",
stringsAsFactors = F)
ling.dom = ling.dom[!is.na(ling.dom$cult.dist),]
ling.dom$family1 = l[match(ling.dom$l1, l$iso2),]$family
ling.dom$family2 = l[match(ling.dom$l2, l$iso2),]$family
fgroup = cbind(ling.dom$family1,ling.dom$family2)
fgroup = apply(fgroup,1,sort)
fgroup
fgroup = cbind(ling.dom$family1,ling.dom$family2)
fgroup
fgroup = apply(fgroup,1,sort)
paste(fgroup)
apply(fgroup,2,paste)
apply(fgroup,2,paste,collapse=":")
tail(cbind(ling.dom$family1,ling.dom$family2))
length(unique(ling.dom$family.group))
length(unique(paste(ling.dom$family1, ling.dom$family2)))
ling.dom$family1 = l[match(ling.dom$l1, l$iso2),]$family
ling.dom$family2 = l[match(ling.dom$l2, l$iso2),]$family
fgroup = cbind(ling.dom$family1,ling.dom$family2)
fgroup = apply(fgroup,1,sort)
ling.dom$family.group = apply(fgroup,2,paste,collapse=":")
length(unique(ling.dom$family.group))
---
title: "Cultural distances: controlling for history"
output: pdf_document
---
```{r echo=F, eval=F}
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/")
```
# Introduction
We compare cultural distances between socieites with linguistic distances between societies, controlling for shared history in two ways.
The first test uses mixed effects modelling.  The pairing of the langauge family of each language (according to Glottolog) is used as a random effect.  That means that the model can capture the likelihood that two languages from the Indo-European langauge family will be more similar to each other than two languages from different langauge families.  The same is done with geographic area according to Autotyp.
The second test controls for history using distances from a phylogenetic tree.  The tree comes from Bouckaert et al. (2012).  Patristic distances between languages are used as a measure of historical distance between societies in a Mantel test.  Note that the Mantel test assumes a strict distance metric, which is not necessarily the case with this data, but there are few other ways to deal with continuous pairwise distances.
# Load libraries
```{r warning=F, message=F}
library(ape)
library(ecodist)
library(lme4)
library(sjPlot)
library(ggplot2)
library(igraph)
library(lattice)
```
# All domains
## Load data
Read the cultural distances:
```{r}
cult = read.csv("../results/EA_distances/CulturalDistances_Long.csv", stringsAsFactors = F)
names(cult) = c("l1","l2","cult.dist")
cultLangs = unique(c(cult$Var1,cult$Var2))
```
Add language family:
```{r}
l = read.csv("../data/FAIR_langauges_glotto_xdid.csv", stringsAsFactors = F)
g = read.csv("../data/glottolog-languoid.csv/languoid.csv", stringsAsFactors = F)
l$family = g[match(l$glotto,g$id),]$family_pk
l$family = g[match(l$family,g$pk),]$name
```
Read the semantic distances
```{r}
ling = read.csv("../data/FAIR/semantic_distances_FAIR.csv", stringsAsFactors = F)
```
Combine the lingusitic and cultural distances
```{r}
cult$l1.iso2 = l[match(cult$l1,l$Language2),]$iso2
cult$l2.iso2 = l[match(cult$l2,l$Language2),]$iso2
fairisos = unique(c(ling$l1,ling$l2))
cultisos = unique(c(cult$l1.iso2, cult$l2.iso2))
cult = cult[(cult$l1.iso2 %in% fairisos) & (cult$l2.iso2 %in% fairisos),]
ling = ling[(ling$l1 %in% cultisos) & (ling$l2 %in% cultisos),]
matches = sapply(1:nrow(ling), function(i){
which(cult$l1.iso2==ling$l1[i] & cult$l2.iso2==ling$l2[i])
})
ling$cult.dist = cult[matches,]$cult.dist
ling$cult.dist.center = scale(ling$cult.dist)
cdc.s = attr(ling$cult.dist.center,"scaled:scale")
cdc.c = attr(ling$cult.dist.center,"scaled:center")
ling$cult.dist.center = as.numeric(ling$cult.dist.center)
ling$family1 = l[match(ling$l1, l$iso2),]$family
ling$family2 = l[match(ling$l2, l$iso2),]$family
ling$area1 = l[match(ling$l1, l$iso2),]$autotyp.area
ling$area2 = l[match(ling$l2, l$iso2),]$autotyp.area
fgroup = cbind(ling$family1,ling$family2)
fgroup = apply(fgroup,1,sort)
ling$family.group = apply(fgroup,2,paste,collapse=":")
agroup = cbind(ling$area1,ling$area2)
agroup = apply(agroup,1,sort)
ling$area.group = apply(agroup,2,paste,collapse=":")
ling$rho.center = scale(ling$rho)
```
Each observation is now assocaited with a langauge family pair:
```{r}
head(ling[,c("l1","l2","rho",'family.group')])
```
And the same is true for area:
```{r}
tail(ling[,c("l1","l2","rho",'area.group')])
```
## LMER models
Mixed effects model, predicting linguistic distances from cultural distances, with random intercept for family and area and random slope for cultural distance for family and area.
We compare a null model to a model with a fixed effect for cultural distance, with random intercepts for family and area, and random slopes for cultural distance by both.
```{r}
m0 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0,m1)
ranef(m1)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(0 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0,m1)
ranef(m1)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center || family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
summary(m1)
ranef(m1)
m0b = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | area.group),
data = ling
)
m1b = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0b,m1b)
ranef(m1b)
ranef(m1)
summary(m1b)
summary(m1)
anova(m0,m1)
m0 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0,m1)
m0b = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | area.group),
data = ling
)
m1b = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0b,m1b)
summary(m1b)
anova(m0,m1)
mD5 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center || family.group) +
(1 + cult.dist.center | imputed_semantic_domain),
data = ling.dom)
mD6 = update(mD5, ~.+cult.dist.center)
anova(mD5,mD6)
ling.dom = read.csv(
"../results/EA_distances/All_Domains_with_ling.csv",
stringsAsFactors = F)
ling.dom = ling.dom[!is.na(ling.dom$cult.dist),]
ling.dom$family1 = l[match(ling.dom$l1, l$iso2),]$family
ling.dom$family2 = l[match(ling.dom$l2, l$iso2),]$family
ling.dom$area1 = l[match(ling.dom$l1, l$iso2),]$autotyp.area
ling.dom$area2 = l[match(ling.dom$l2, l$iso2),]$autotyp.area
# Paste language family names together,
# but order shouldn't matter, so sort first
fgroup = cbind(ling.dom$family1,ling.dom$family2)
fgroup = apply(fgroup,1,sort)
ling.dom$family.group = apply(fgroup,2,paste,collapse=":")
agroup = cbind(ling.dom$area1,ling.dom$area2)
agroup = apply(agroup,1,sort)
ling.dom$area.group = apply(agroup,2,paste,collapse=":")
```
Center the data:
```{r}
ling.dom$cult.dist.center = scale(ling.dom$cult.dist)
ling.dom$rho.center = scale(ling.dom$rho)
```
mD5 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center || family.group) +
(1 + cult.dist.center | imputed_semantic_domain),
data = ling.dom)
mD6 = update(mD5, ~.+cult.dist.center)
anova(mD5,mD6)
mD5 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center || family.group) +
(1 | imputed_semantic_domain),
data = ling.dom)
mD6 = update(mD5, ~.+cult.dist.center)
anova(mD5,mD6)
mD5 = lmer(
rho.center ~ 1 +
(1 | family.group) +
(1 | imputed_semantic_domain),
data = ling.dom)
mD6 = update(mD5, ~.+cult.dist.center)
anova(mD5,mD6)
mx = lmer(
rho.center ~ 1 +
(1 +cult.dist.center| family.group) +
(1 | area.group) +
(1 | imputed_semantic_domain),
data = ling.dom)
mx2 = lmer(
rho.center ~ 1 +
(1 +cult.dist.center| family.group) +
(1 | area.group) +
(1 + cult.dist.center| imputed_semantic_domain),
data = ling.dom)
anova(mx,mx2)
?update
library(reshape2)
library(ggplot2)
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/")
load("../data/EA_imputed/preImputed.Rdat")
soc.id = eadx[,1]
# take out socid
eadx = eadx[,-1]
getAccuracy = function(f){
#print(f)
load(f)
if(nrow(eadx2)==nrow(eadx)){
test.arrInd = test.arrInd[test.arrInd[,1]!=0,]
accuracy = sum(eadx2[test.arrInd] == eadx[test.arrInd])/nrow(test.arrInd)
# baseline
baseline = replicate(100,getRandomBaseline(test.arrInd,eadx,eadx2))
z = (accuracy - mean(baseline))/sd(baseline)
baseline2 = replicate(100,getRandomBaseline_totallyRandom(test.arrInd,eadx,eadx2))
z2 = (accuracy - mean(baseline2))/sd(baseline2)
return(c(accuracy=accuracy, baseline=mean(baseline), z=z,
baseline2 = mean(baseline2), z2 = z2))
} else{
print("Row numbers don't match")
}
}
getRandomBaseline = function(test.arrInd, eadx, eadx2){
# imputation by random sampling
randomBaseline =
sapply(test.arrInd[,2], function(X){
sx = eadx2[,X]
sx = sx[!is.na(sx)]
sample(sx,1)
})
sum(randomBaseline == eadx[test.arrInd])/nrow(test.arrInd)
}
getRandomBaseline_totallyRandom = function(test.arrInd, eadx, eadx2){
# imputation by random sampling of set (no frequencies)
randomBaseline =
sapply(test.arrInd[,2], function(X){
sx = eadx2[,X]
sx = sx[!is.na(sx)]
sample(unique(sx),1)
})
sum(randomBaseline == eadx[test.arrInd])/nrow(test.arrInd)
}
getFolderAccuracy = function(folder,prefix){
accuracy.orig = sapply(
paste0(folder,
list.files(folder,prefix)),
getAccuracy)
accuracy.orig = as.data.frame(t(accuracy.orig))
return(c(mean(accuracy.orig$accuracy,na.rm=T),
(mean(accuracy.orig$baseline,na.rm=T)),
(mean(accuracy.orig$z,na.rm=T)),
(mean(accuracy.orig$baseline2,na.rm=T)),
(mean(accuracy.orig$z2,na.rm=T))))
}
########################
getFolderAccuracy("../results/imputationTests/testFAIR_15/","imputeTest_FAIR_15_*")
