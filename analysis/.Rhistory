return(concepticon$alternative_labels[[gloss2]])
}
return(c("",""))
}
concepts = sapply(d$GLOSS,findConcept)
d$CONCEPT = concepts[2,]
d$CONCEPTID = concepts[1,]
d[d$CONCEPT=="",]$GLOSS
extra = c("arm/ hand"=1673,
"wheat(husked)"=1077,
"to give " = 1447,
"to kill " = 1417,
"to walk " = 1443,
"to run/ run" = 1519,
"to go /go" = 695,
"to speak/ speak" =1623,
"to hear/hear/listen" = 1608,
"to look/look"   =1819,
"flour (eaten as food)" =1594,
"hailstone"  =609,
"to uproot" = 9991,
"bond" = 1917,
"beam (main wooden beam)" =1132	,
"rope used to tie animals" = 1218	,
"plank" = 1227,
"plough (noun)" = 2154,
"lid (cover)" =2319,
"manure" = 2057,
"grains" = 605)
d$CONCEPTID[match(names(extra),d$GLOSS)] = extra
d$CONCEPT[match(names(extra),d$GLOSS)] = concepticonGloss[2,match(as.character(extra),concepticonGloss[1,])]
d[is.na(d$CONCEPT),]$CONCEPT = ""
d[is.na(d$CONCEPT),]$CONCEPTID = "9991"
d[d$GLOSS=="to uproot",]
write.csv("../data/reference/finalWordList.csv",row.names = F)
write.csv(d,"../data/reference/finalWordList.csv",row.names = F)
finalConcepts = read.csv("../data/reference/finalWordList.csv",stringsAsFactors = F)
head(finalConcepts)
finalConcepts = read.csv("../data/reference/finalWordList.csv",stringsAsFactors = F)
finalConceptList = unique(finalConcepts$CONCEPT)
library(rjson)
library(stringdist)
library(openxlsx)
try(setwd("~/Documents/Funding/InternationalStrategicFund/project/processing/"))
readTSV = function(f){
read.table(f,quote = "",stringsAsFactors = F, encoding = 'utf-8',fileEncoding = 'utf-8',sep="\t", header = T)
}
concepticon = fromJSON(file ="../data/reference/concepticon_conceptset.json/conceptset.json")
languageFiles = c(
tsum="../data/processed/Tsum.tsv",
nubri="../data/processed/Nubri.tsv",
nubri_namrung = "../data/processed/Nubri_Namrung.tsv",
nubri_lhi = "../data/processed/Nubri_Lhi.tsv",
nubri_lho = "../data/processed/Nubri_Lho.tsv",
nubri_sama = "../data/processed/Nubri_Sama.tsv",
nubri_sho = "../data/processed/Nubri_Sho.tsv",
gyalsumdo = "../data/processed/Gyalsumdo.tsv",
jirel = "../data/processed/Jirel.tsv",
lowa = "../data/processed/Lowa.tsv",
yolmo = "../data/processed/Yolmo.tsv",
kagate = "../data/processed/Kagate.tsv")
# Load data
allData = list()
for(l in names(languageFiles)){
allData[[l]] = readTSV(languageFiles[l])
}
# advancedMatch= F
# if(advancedMatch){
#   tsum = readTSV("../data/processed/Tsum_advancedMatch.tsv")
#   tsum[tsum$CONCEPT=="",]$CONCEPT = tsum[tsum$CONCEPT=="",]$CONCEPTICON_GLOSS
#   nubri = readTSV("../data/processed/Nubri_advancedMatch.tsv")
#   nubri[nubri$CONCEPT=="",]$CONCEPT = nubri[nubri$CONCEPT=="",]$CONCEPTICON_GLOSS
#   gyalsumdo = readTSV("../data/processed/Gyalsumdo_advancedMatch.tsv")
#   gyalsumdo[gyalsumdo$CONCEPT=="",]$CONCEPT = gyalsumdo[gyalsumdo$CONCEPT=="",]$CONCEPTICON_GLOSS
#   jirel = readTSV("../data/processed/Jirel_advancedMatch.tsv")
#   jirel[jirel$CONCEPT=="",]$CONCEPT = jirel[jirel$CONCEPT=="",]$CONCEPTICON_GLOSS
#   lowa = readTSV("../data/processed/Lowa_advancedMatch.tsv")
#   lowa[lowa$CONCEPT=="",]$CONCEPT = lowa[lowa$CONCEPT=="",]$CONCEPTICON_GLOSS
#   yolmo = readTSV("../data/processed/Yolmo_advancedMatch.tsv")
#   yolmo[yolmo$CONCEPT=="",]$CONCEPT = yolmo[yolmo$CONCEPT=="",]$CONCEPTICON_GLOSS
#
# }
for(l in names(languageFiles)){
write.xlsx(allData[[l]],file =
gsub("\\.tsv",".xlsx",languageFiles[l]))
}
# Calculate stats
swadesh100 = read.csv("../data/reference/Swadesh1964_100.csv",stringsAsFactors = F, encoding = "utf-8",fileEncoding = 'utf-8')
dunn207 = read.csv("../data/reference/Dun_2012_207.tab", stringsAsFactors = F, encoding = 'utf-8', fileEncoding = "utf-8", sep="\t",quote="")
#finalConceptList = unique(allData[["jirel"]]$CONCEPT)
finalConcepts = read.csv("../data/reference/finalWordList.csv",stringsAsFactors = F)
finalConceptList = unique(finalConcepts$CONCEPT)
stats = data.frame(
language = names(languageFiles),
numEntries =
sapply(allData,function(X){length(X$CONCEPT)}),
numConceptsMatchedToConcepticon =
sapply(allData,function(X){length(unique(X$CONCEPT))}),
numSwadesh100ConceptsMatchedToConcepticon =
sapply(allData,function(X){sum(unique(X$CONCEPT) %in% swadesh100$Parameter)}),
numJirel208ConceptsMatchedToConcepticon =
sapply(allData,function(X){sum(finalConceptList %in% unique(X$CONCEPT))})
)
stats$progress = paste0(round(100*(stats$numJirel208ConceptsMatchedToConcepticon/length(finalConceptList)),0),"%")
write.xlsx(stats,"../data/processed/progress.xlsx")
# Find missing concepts
findMissingConcepts = function(d){
missing = data.frame(
DOCULECT = d$DOCULECT[1],
CONCEPT = finalConceptList[!finalConceptList %in% d$CONCEPT])
missing$CONCEPTID = sapply(concepticon$conceptset_labels[missing$CONCEPT],head,n=1)
return(missing)
}
missing = data.frame()
for(l in names(allData)){
if(l!="jirel"){
missing = rbind(missing,
findMissingConcepts(allData[[l]]))
}
}
write.csv(missing,file="../data/processed/MissingConcepts.csv", fileEncoding = 'utf-8')
library(rjson)
library(stringdist)
library(openxlsx)
try(setwd("~/Documents/Funding/InternationalStrategicFund/project/processing/"))
readTSV = function(f){
read.table(f,quote = "",stringsAsFactors = F, encoding = 'utf-8',fileEncoding = 'utf-8',sep="\t", header = T)
}
concepticon = fromJSON(file ="../data/reference/concepticon_conceptset.json/conceptset.json")
languageFiles = c(
tsum="../data/processed/Tsum.tsv",
nubri="../data/processed/Nubri.tsv",
nubri_namrung = "../data/processed/Nubri_Namrung.tsv",
nubri_lhi = "../data/processed/Nubri_Lhi.tsv",
nubri_lho = "../data/processed/Nubri_Lho.tsv",
nubri_sama = "../data/processed/Nubri_Sama.tsv",
nubri_sho = "../data/processed/Nubri_Sho.tsv",
gyalsumdo = "../data/processed/Gyalsumdo.tsv",
jirel = "../data/processed/Jirel.tsv",
lowa = "../data/processed/Lowa.tsv",
yolmo = "../data/processed/Yolmo.tsv",
kagate = "../data/processed/Kagate.tsv")
# Load data
allData = list()
for(l in names(languageFiles)){
allData[[l]] = readTSV(languageFiles[l])
}
# advancedMatch= F
# if(advancedMatch){
#   tsum = readTSV("../data/processed/Tsum_advancedMatch.tsv")
#   tsum[tsum$CONCEPT=="",]$CONCEPT = tsum[tsum$CONCEPT=="",]$CONCEPTICON_GLOSS
#   nubri = readTSV("../data/processed/Nubri_advancedMatch.tsv")
#   nubri[nubri$CONCEPT=="",]$CONCEPT = nubri[nubri$CONCEPT=="",]$CONCEPTICON_GLOSS
#   gyalsumdo = readTSV("../data/processed/Gyalsumdo_advancedMatch.tsv")
#   gyalsumdo[gyalsumdo$CONCEPT=="",]$CONCEPT = gyalsumdo[gyalsumdo$CONCEPT=="",]$CONCEPTICON_GLOSS
#   jirel = readTSV("../data/processed/Jirel_advancedMatch.tsv")
#   jirel[jirel$CONCEPT=="",]$CONCEPT = jirel[jirel$CONCEPT=="",]$CONCEPTICON_GLOSS
#   lowa = readTSV("../data/processed/Lowa_advancedMatch.tsv")
#   lowa[lowa$CONCEPT=="",]$CONCEPT = lowa[lowa$CONCEPT=="",]$CONCEPTICON_GLOSS
#   yolmo = readTSV("../data/processed/Yolmo_advancedMatch.tsv")
#   yolmo[yolmo$CONCEPT=="",]$CONCEPT = yolmo[yolmo$CONCEPT=="",]$CONCEPTICON_GLOSS
#
# }
for(l in names(languageFiles)){
write.xlsx(allData[[l]],file =
gsub("\\.tsv",".xlsx",languageFiles[l]))
}
# Calculate stats
swadesh100 = read.csv("../data/reference/Swadesh1964_100.csv",stringsAsFactors = F, encoding = "utf-8",fileEncoding = 'utf-8')
dunn207 = read.csv("../data/reference/Dun_2012_207.tab", stringsAsFactors = F, encoding = 'utf-8', fileEncoding = "utf-8", sep="\t",quote="")
#finalConceptList = unique(allData[["jirel"]]$CONCEPT)
finalConcepts = read.csv("../data/reference/finalWordList.csv",stringsAsFactors = F)
finalConceptList = unique(finalConcepts$CONCEPT)
stats = data.frame(
language = names(languageFiles),
numEntries =
sapply(allData,function(X){length(X$CONCEPT)}),
numConceptsMatchedToConcepticon =
sapply(allData,function(X){length(unique(X$CONCEPT))}),
numSwadesh100ConceptsMatchedToConcepticon =
sapply(allData,function(X){sum(unique(X$CONCEPT) %in% swadesh100$Parameter)}),
numFinalWordListConceptsMatchedToConcepticon =
sapply(allData,function(X){sum(finalConceptList %in% unique(X$CONCEPT))})
)
stats$progress = paste0(round(100*(stats$numFinalWordListConceptsMatchedToConcepticon/length(finalConceptList)),0),"%")
write.xlsx(stats,"../data/processed/progress.xlsx")
# Find missing concepts
findMissingConcepts = function(d){
missing = data.frame(
DOCULECT = d$DOCULECT[1],
CONCEPT = finalConceptList[!finalConceptList %in% d$CONCEPT])
missing$CONCEPTID = sapply(concepticon$conceptset_labels[missing$CONCEPT],head,n=1)
return(missing)
}
missing = data.frame()
for(l in names(allData)){
if(l!="jirel"){
missing = rbind(missing,
findMissingConcepts(allData[[l]]))
}
}
write.csv(missing,file="../data/processed/MissingConcepts.csv", fileEncoding = 'utf-8')
table(missing$CONCEPT,missing$DOCULECT)
rowSums(table(missing$CONCEPT,missing$DOCULECT))
sort(rowSums(table(missing$CONCEPT,missing$DOCULECT)))
---
title: "Cultural distances: controlling for history"
output: pdf_document
---
```{r echo=F, eval=F}
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/")
```
# Introduction
We compare cultural distances between socieites with linguistic similarities between societies, controlling for shared history in two ways.
The first test uses mixed effects modelling.  The pairing of the language family of each language (according to Glottolog) is used as a random effect.  That means that the model can capture the likelihood that two languages from the Indo-European language family will be more similar to each other than two languages from different language families.  The same is done with geographic area according to Autotyp.
The second test controls for history using distances from a phylogenetic tree.  The tree comes from Bouckaert et al. (2012).  Patristic distances between languages are used as a measure of historical distance between societies in a Mantel test.  Note that the Mantel test assumes a strict distance metric, which is not necessarily the case with this data, but there are few other ways to deal with continuous pairwise distances.
# Load libraries
```{r warning=F, message=F}
library(ape)
library(ecodist)
library(lme4)
library(sjPlot)
library(ggplot2)
library(igraph)
library(lattice)
```
# All domains
## Load data
Read the cultural distances:
```{r}
cult = read.csv("../results/EA_distances/CulturalDistances_Long.csv", stringsAsFactors = F)
names(cult) = c("l1","l2","cult.dist")
cultLangs = unique(c(cult$Var1,cult$Var2))
```
Add language family:
```{r}
l = read.csv("../data/FAIR_langauges_glotto_xdid.csv", stringsAsFactors = F)
g = read.csv("../data/glottolog-languoid.csv/languoid.csv", stringsAsFactors = F)
l$family = g[match(l$glotto,g$id),]$family_pk
l$family = g[match(l$family,g$pk),]$name
```
Read the semantic distances
```{r}
ling = read.csv("../data/FAIR/wikipedia-by-language-pair-mean-rho", stringsAsFactors = F)
ling = read.csv("../data/FAIR/wikipedia-by-language-pair-mean-rho.csv", stringsAsFactors = F)
cult$l1.iso2 = l[match(cult$l1,l$Language2),]$iso2
cult$l2.iso2 = l[match(cult$l2,l$Language2),]$iso2
fairisos = unique(c(ling$l1,ling$l2))
cultisos = unique(c(cult$l1.iso2, cult$l2.iso2))
cult = cult[(cult$l1.iso2 %in% fairisos) & (cult$l2.iso2 %in% fairisos),]
ling = ling[(ling$l1 %in% cultisos) & (ling$l2 %in% cultisos),]
matches = sapply(1:nrow(ling), function(i){
which(cult$l1.iso2==ling$l1[i] & cult$l2.iso2==ling$l2[i])
})
ling$cult.dist = cult[matches,]$cult.dist
ling$cult.dist.center = scale(ling$cult.dist)
cdc.s = attr(ling$cult.dist.center,"scaled:scale")
cdc.c = attr(ling$cult.dist.center,"scaled:center")
ling$cult.dist.center = as.numeric(ling$cult.dist.center)
ling$family1 = l[match(ling$l1, l$iso2),]$family
ling$family2 = l[match(ling$l2, l$iso2),]$family
ling$area1 = l[match(ling$l1, l$iso2),]$autotyp.area
ling$area2 = l[match(ling$l2, l$iso2),]$autotyp.area
fgroup = cbind(ling$family1,ling$family2)
fgroup = apply(fgroup,1,sort)
ling$family.group = apply(fgroup,2,paste,collapse=":")
agroup = cbind(ling$area1,ling$area2)
agroup = apply(agroup,1,sort)
ling$area.group = apply(agroup,2,paste,collapse=":")
ling$rho.center = scale(ling$rho)
head(ling[,c("l1","l2","rho",'family.group')])
ling$family1
sum(is.na(ling$family1))
sum(is.na(ling$area1))
tail(ling[,c("l1","l2","rho",'area.group')])
m0 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0,m1)
summary(m1)
gx = sjp.lmer(m1,'pred','cult.dist.center',
prnt.plot = F)
gx$plot$data$y = gx$plot$data$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
gx$plot$data$resp.y = gx$plot$data$resp.y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
gx$plot$data$x = gx$plot$data$x *
cdc.s +cdc.c
gx = gx$plot + coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8)) +
xlab("Cultural distance") +
ylab("Semantic alignment") +
ggtitle("")
gx
sjp.lmer(m1,'re', sort.est = "cult.dist.center")
px = sjp.lmer(m1,'rs.ri', prnt.plot = F)
dx = px$plot[[1]]$data
dx$x = dx$x * cdc.s + cdc.c
dx$y = dx$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
ggplot(dx,aes(x,y)) +
geom_point(data=ling,
mapping=aes(x=as.numeric(cult.dist),
y=as.numeric(rho))) +
geom_line(mapping = aes(colour=grp)) +
xlab("Cultural distance")+
ylab("Linguistic similarity") +
ggtitle("Language family pair random effects") +
coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
dx = px$plot[[2]]$data
dx$x = dx$x * cdc.s + cdc.c
dx$y = dx$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
ggplot(dx,aes(x,y)) +
geom_point(data=ling,
mapping=aes(x=as.numeric(cult.dist),
y=as.numeric(rho))) +
geom_line(mapping = aes(colour=grp)) +
xlab("Cultural distance")+
ylab("Linguistic similarity") +
ggtitle("Area pair random effects") +
coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
# combine the distances for linguistic and cultural
# features for each domain
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/processing/")
l = read.csv("../data/FAIR_langauges_glotto_xdid.csv",
stringsAsFactors = F)
combineCultAndLingDistances = function(inputFile,outputFile){
ling.domain = read.csv(inputFile, stringsAsFactors = F)
ling.domain$cult.dist = NA
ling.langs = unique(c(ling.domain$l1,
ling.domain$l2))
domains = unique(ling.domain$imputed_semantic_domain)
for(dom in domains){
dom.filename = paste0('../results/EA_distances/',
gsub(" ","_",dom),"_long.csv")
if(file.exists(dom.filename)){
print(dom)
dx = read.csv(dom.filename, stringsAsFactors = F)
dx$l1 = l[match(dx$Var1,l$Language2),]$iso2
dx$l2 = l[match(dx$Var2,l$Language2),]$iso2
dx = dx[dx$l1 %in% ling.langs & dx$l2 %in% ling.langs,]
lx = ling.domain$imputed_semantic_domain==dom
ling.domain[lx,]$cult.dist = dx[match(
paste(ling.domain[lx,]$l1,ling.domain[lx,]$l2),
paste(dx$l1,dx$l2)
),]$value
}
}
ling.domain = ling.domain[,names(ling.domain)!="X"]
write.csv(ling.domain,outputFile, row.names = F)
}
#combineCultAndLingDistances(
#"../data/FAIR/semantic_distances_by_domain_extended.csv",
#"../results/EA_distances/All_Domains_with_ling.csv")
combineCultAndLingDistances(
"../data/FAIR/wikipedia-by-language-pair-mean-rho.csv",
"../results/EA_distances/wikipedia_All_Domains_with_ling.csv")
---
title: "Cultural distances: controlling for history"
output: pdf_document
---
```{r echo=F, eval=F}
setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/")
```
# Introduction
We compare cultural distances between socieites with linguistic similarities between societies, controlling for shared history in two ways.
The first test uses mixed effects modelling.  The pairing of the language family of each language (according to Glottolog) is used as a random effect.  That means that the model can capture the likelihood that two languages from the Indo-European language family will be more similar to each other than two languages from different language families.  The same is done with geographic area according to Autotyp.
The second test controls for history using distances from a phylogenetic tree.  The tree comes from Bouckaert et al. (2012).  Patristic distances between languages are used as a measure of historical distance between societies in a Mantel test.  Note that the Mantel test assumes a strict distance metric, which is not necessarily the case with this data, but there are few other ways to deal with continuous pairwise distances.
# Load libraries
```{r warning=F, message=F}
library(ape)
library(ecodist)
library(lme4)
library(sjPlot)
library(ggplot2)
library(igraph)
library(lattice)
```
# All domains
## Load data
Read the cultural distances:
```{r}
cult = read.csv("../results/EA_distances/CulturalDistances_Long.csv", stringsAsFactors = F)
names(cult) = c("l1","l2","cult.dist")
cultLangs = unique(c(cult$Var1,cult$Var2))
```
Add language family:
```{r}
l = read.csv("../data/FAIR_langauges_glotto_xdid.csv", stringsAsFactors = F)
g = read.csv("../data/glottolog-languoid.csv/languoid.csv", stringsAsFactors = F)
l$family = g[match(l$glotto,g$id),]$family_pk
l$family = g[match(l$family,g$pk),]$name
```
Read the semantic distances
```{r}
ling = read.csv("../data/FAIR/subtitles-by-language-pair-mean-rho.csv", stringsAsFactors = F)
```
Combine the lingusitic and cultural distances
```{r}
cult$l1.iso2 = l[match(cult$l1,l$Language2),]$iso2
cult$l2.iso2 = l[match(cult$l2,l$Language2),]$iso2
fairisos = unique(c(ling$l1,ling$l2))
cultisos = unique(c(cult$l1.iso2, cult$l2.iso2))
cult = cult[(cult$l1.iso2 %in% fairisos) & (cult$l2.iso2 %in% fairisos),]
ling = ling[(ling$l1 %in% cultisos) & (ling$l2 %in% cultisos),]
matches = sapply(1:nrow(ling), function(i){
which(cult$l1.iso2==ling$l1[i] & cult$l2.iso2==ling$l2[i])
})
ling$cult.dist = cult[matches,]$cult.dist
ling$cult.dist.center = scale(ling$cult.dist)
cdc.s = attr(ling$cult.dist.center,"scaled:scale")
cdc.c = attr(ling$cult.dist.center,"scaled:center")
ling$cult.dist.center = as.numeric(ling$cult.dist.center)
ling$family1 = l[match(ling$l1, l$iso2),]$family
ling$family2 = l[match(ling$l2, l$iso2),]$family
ling$area1 = l[match(ling$l1, l$iso2),]$autotyp.area
ling$area2 = l[match(ling$l2, l$iso2),]$autotyp.area
fgroup = cbind(ling$family1,ling$family2)
fgroup = apply(fgroup,1,sort)
ling$family.group = apply(fgroup,2,paste,collapse=":")
agroup = cbind(ling$area1,ling$area2)
agroup = apply(agroup,1,sort)
ling$area.group = apply(agroup,2,paste,collapse=":")
ling$rho.center = scale(ling$rho)
head(ling[,c("l1","l2","rho",'family.group')])
tail(ling[,c("l1","l2","rho",'area.group')])
sum(is.na(ling$area1))
sum(is.na(ling$area2))
sum(is.na(ling$family1))
sum(is.na(ling$family2))
m0 = lmer(
rho.center ~ 1 +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
m1 = lmer(
rho.center ~ 1 +
cult.dist.center +
(1 + cult.dist.center | family.group) +
(1 + cult.dist.center | area.group),
data = ling
)
anova(m0,m1)
gx = sjp.lmer(m1,'pred','cult.dist.center',
prnt.plot = F)
gx$plot$data$y = gx$plot$data$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
gx$plot$data$resp.y = gx$plot$data$resp.y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
gx$plot$data$x = gx$plot$data$x *
cdc.s +cdc.c
gx = gx$plot + coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8)) +
xlab("Cultural distance") +
ylab("Semantic alignment") +
ggtitle("")
gx
sjp.lmer(m1,'re', sort.est = "cult.dist.center")
px = sjp.lmer(m1,'rs.ri', prnt.plot = F)
dx = px$plot[[1]]$data
dx$x = dx$x * cdc.s + cdc.c
dx$y = dx$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
ggplot(dx,aes(x,y)) +
geom_point(data=ling,
mapping=aes(x=as.numeric(cult.dist),
y=as.numeric(rho))) +
geom_line(mapping = aes(colour=grp)) +
xlab("Cultural distance")+
ylab("Linguistic similarity") +
ggtitle("Language family pair random effects") +
coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
dx = px$plot[[2]]$data
dx$x = dx$x * cdc.s + cdc.c
dx$y = dx$y *
attr(ling$rho.center,"scaled:scale") +
attr(ling$rho.center,"scaled:center")
ggplot(dx,aes(x,y)) +
geom_point(data=ling,
mapping=aes(x=as.numeric(cult.dist),
y=as.numeric(rho))) +
geom_line(mapping = aes(colour=grp)) +
xlab("Cultural distance")+
ylab("Linguistic similarity") +
ggtitle("Area pair random effects") +
coord_cartesian(ylim=c(0.1,0.6),
xlim=c(0.2,0.8))
paste0("../results/stats/",datasetName,"/CulturalDistance_Rho_Graph.pdf")
datasetName = "subtitles"
paste0("../results/stats/",datasetName,"/CulturalDistance_Rho_Graph.pdf")
