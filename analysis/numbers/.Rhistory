h = read.csv("data/numbers/NumberHomophones.csv",
stringsAsFactors = F,encoding = "UTF-8",fileEncoding = "UTF-8")
h$code = paste(h$l,h$word)
numbers$homophone = (paste(numbers$l1,numbers$wordform_l1) %in% h$code) |
(paste(numbers$l2,numbers$wordform_l2) %in% h$code)
numbers$lang_pair.f = factor(numbers$lang_pair)
# Frequency difference (already in log scale)
numbers$freqDiff = abs(numbers$freq_l1-numbers$freq_l2)
# 78 frequency observations (3%) are missing, so impute:
freqM = bam(I(1+freqDiff)~
#s(number_numeric) +
s(lang_pair.f,bs='re') +
s(editdistance) +
s(hist.dist2),
family = Gamma(link="identity"),
data = numbers[!is.na(numbers$freqDiff),])
freqMPred = predict(freqM,newdata=numbers)-1
#plot(freqMPred,numbers$freqDiff)
#abline(0,1)
numbers[is.na(numbers$freqDiff),]$freqDiff =
freqMPred[is.na(numbers$freqDiff)]
```
Group data by language:
```{r}
langAverages = data.frame()
for(l in unique(c(numbers$l1,numbers$l2))){
dx = numbers[numbers$l1==l | numbers$l2==l,]
langAverages = rbind(langAverages,
data.frame(
local_alignment = dx$local_alignment,
number_numeric = dx$number_numeric,
l = l,
l1 = dx$name_l1,
l2 = dx$name_l2))
}
langAverages$l = factor(langAverages$l,
levels = names(sort(tapply(langAverages$local_alignment,langAverages$l,mean))))
```
# Overview
## Numeric value
Plot by numeric value:
```{r}
ggplot(numbers,aes(x=factor(number_numeric),y=local_alignment)) + geom_boxplot()
```
-  1 has a low numeric alignment.
-  Alignment rises 2-3.
-  7 has a lower alignment.
-  Drop in alignment for 100, 1000.
-  Outliers for 9.
1 and 2 are often sources for grammaticalised indefinite/duel markers (Giv√≥n, 1981).
The slight decline from 10 to 1000 could be due to the declining frequency of occurances of these numbers (Dehaene & Mehler, 1992), which might affect convergence on meanings, but more direcly would affect the co-occurance statistics.
What's driving the difference with 7 and 9? 7 might be linked to there being 7 days in the week, so semantic differences in time might be reflected. Let's look at individual languages:
```{r}
la7 = langAverages[langAverages$number_numeric==7,]
la7$l = factor(la7$l,levels=names(sort(tapply(
la7$local_alignment,la7$l,mean))))
ggplot(la7,aes(x=l,y=local_alignment)) + geom_boxplot()
```
The plot above shows that the main difference is being driven by comparisons with Hungarian. All the outliers around 0.25 are comparisons with Hungarian. This might be because Hungarian is a Uralic language, but maybe also because the Hungarian word for '7' also directly means "week".
Ukrainian is also low. We note that forms for 7 and 8 are very similar in Ukrainian (7 = sim and 8 = visim).
What are the outliers for 9? These all include comparisons to French:
```{r}
numbers[numbers$number_numeric==9 & numbers$local_alignment<0.25,c("l1",'l2',"local_alignment")]
```
This may be because French 9 ("neuf") can mean '9' or "new".  We used the North Euralex dictionary to find numbers which have homophones.  See the file `NumberHomophones.csv`. This also includes the Hungarian '7' discussed below.
```{r}
h[,c("l","number","otherMeanings")]
```
The plot below shows the distribution of non-homophones, with homophones drawn as dots. For 7 and 9, these fall outside of the general distribution, but there are several other cases where homophones look similar to the rest of the distribution:
```{r}
ggplot(numbers[!numbers$homophone,],
aes(y=local_alignment,
x=factor(number_numeric))) +
geom_violin() +
geom_point(data=numbers[numbers$homophone,],
aes(y=local_alignment,
x=factor(number_numeric)),
colour="red") +
myThemeBasic
```
## Number line
We can look at the importance of the different ways that numeral systems are compased. We use data from Calude & Verkerk (2016), and identify whether two languages have the same system for forming a particular numeral.
In our sample, all numbers below 10 are atoms, so there are only differences above 10. For example, English uses a unique atom to represent 12 (/twelv/), while Bulgarian uses a form that is composed of "2+10" (2 = dve, 10 = deset, 12 = dvanadeset).
It looks like there's an interaction between numeral typoogy and numeric value:
```{r}
ggplot(numbers[numbers$number_numeric>10,],
aes(y=local_alignment,
x=factor(number_numeric),
colour=sameNumeralTypology)) +
geom_boxplot()
```
## Variation by language
The plot below shows the overview of comparisons by language:
```{r}
ggplot(langAverages,aes(x=l,y=local_alignment)) + geom_boxplot()
```
Danish seems to have a lower average.  We note that some Danish 'crowns' are irregular (50,60,70,80,90).  These pick out the outliers (dots are danish, blue dots are irregualr, violin plots are the rest of the data):
```{r}
ggplot(numbers[!numbers$is_danish,],
aes(x=factor(number_numeric),y=local_alignment)) +
geom_violin() + myThemeBasic +
geom_point(data=numbers[numbers$is_danish,],
aes(colour=danish_irregular))
```
## Variation by frequency
Semantic alignment by frequency (brighter colours are higher numeri vlaues):
```{r}
ggplot(numbers,
aes(x=freqDiff,y=local_alignment,colour=log10(number_numeric))) +
geom_point() +
stat_smooth() +
theme(legend.position = 'none') +
myThemeBasic + xlab("Frequency difference") +
ylab("Local alignment")
```
There appears to be a slight effect for larger frequency differences to be associated with lower alignment.
\newpage
# Decision tree
We use a decision tree to explore the data and find coherent clusters in the data.  We try to predict local alignment based on various properties.
First we show that Uralic "7s" are a coherent cluster:  A decision tree divides the data into categories that represent '1', '1000', and then combines divisions in the numeric value with 'isUralic' to find the cluster of Uralic 7s:
```{r}
rt = REEMtree(local_alignment~
number_numeric + isUralic,
random = ~1|lang_pair,
data=numbers)
rp.rt = tree(rt)
rp.rt$model = numbers
rpart.plot(rp.rt, type=4, branch.lty=1, clip.facs = F, box.palette="RdYlGn")
```
To help this, we include an explicit factor for seven. The final factors in the mode are:
-  numeric value
-  is a Uralic language
-  is a Danish irregular
-  is '7'
-  has a homophone
-  the 'n' variable: number of comparisons possible between language pairs in the whole corpus
-  whether the numbers have the same underling compositional structure
```{r eval=F}
set.seed(1283)
rt = REEMtree(local_alignment~
number_numeric + isUralic +
danish_irregular + hist.dist2 +
seven + n + sameNumeralTypology + homophone,
random = ~1|lang_pair,
data=numbers,
MaxIterations=1000000)
rp.rt = tree(rt)
rp.rt$model = numbers
plot1 = rpart.plot(rp.rt, type=4, branch.lty=1, clip.facs = F, box.palette="RdYlGn")
cluster = factor(rp.rt$where,
labels = c("One",
"Hungarian 7",
"French 9",
"Small\nhomophones",
"Large\nhomophones",
"100,1000",
"Danish\nirregulars",
"Two",
"3-90"))
plot2 = ggplot(numbers,aes(y=local_alignment,
x=cluster)) +
geom_violin() + myThemeBasic +
xlab("") + ylab("Semantic alginment")
pdf("results/DecisionTree.pdf",width=12,height=8)
layout(t(t(c(1,2))), heights=c(2.5,1))
par(mar=c(1,10,1,1))
rpart.plot(rp.rt, type=4, branch.lty=2,
clip.facs = F, box.palette="RdYlGn",
mar=c(1,4,1,1.5),cex = 1.2,split.yshift=1)
plot.new()
vps <- baseViewports()
pushViewport(vps$figure)
vp1 <-plotViewport(c(0,0,0,0))
print(plot2,vp = vp1)
dev.off()
varimp = sort(rt$Tree$variable.importance)
varimp.plot = ggplot(data.frame(importance=varimp,
variable=factor(names(varimp),levels = names(varimp))),
aes(y=importance,x=variable))+
geom_col() + coord_flip()
pdf("results/VarImp.pdf")
varimp.plot
dev.off()
```
![](results/DecisionTree.pdf)
![](results/VarImp.pdf)
# Run a GAM
Convert lang_pair to factor and scale variables:
```{r}
numbers$lang_pair = factor(numbers$lang_pair)
numbers$number_numeric.log = log(numbers$number_numeric)
numbers$number_numeric.log.scaled = scale(numbers$number_numeric.log)
numbers$local_alignment.center = scale(numbers$local_alignment,scale=F)
numbers$isUralic = factor(numbers$family_l1 == "Uralic" | numbers$family_l2 == "Uralic")
numbers$differentNumeralTypology = factor(!numbers$sameNumeralTypology)
numbers$seven = factor(numbers$seven)
numbers$danish_irregular = factor(numbers$danish_irregular)
numbers$homophone = factor(numbers$homophone)
```
We start by looking at a simple model that has a random effect for langauge pair and a main smooth term for number. Note that since this is a non-linear model, a random effect for numeric value is very similar to a fully articulated smooth slope, so we just model numeric as a fixed effect.
```{r}
m0 = bam(local_alignment.center ~
s(lang_pair,bs='re') +
s(number_numeric.log.scaled),
data = numbers)
```
We plot the fit of the model below:
```{r echo=F}
modelToPlot = m0
plot.new()
px = plotGAMSignificantSlopes(
m0,
'number_numeric.log.scaled',
'Number')
# Model is in scaled space, so transform back:
# Undo scale
px$curve.x = px$curve.x *
attr(numbers$number_numeric.log.scaled,"scaled:scale") +
attr(numbers$number_numeric.log.scaled,"scaled:center")
# Undo log
px$curve.x = exp(px$curve.x)
# Function to do the same for y
transform_y = function(y){
y + attr(numbers$local_alignment.center,"scaled:center")
}
intercept = coef(modelToPlot)[1]
px$curve.y = transform_y(px$curve.y + intercept)
px$curve.y.upper = transform_y(px$curve.y.upper+ intercept)
px$curve.y.lower = transform_y(px$curve.y.lower + intercept)
px$m2.dsig.incr = transform_y(px$m2.dsig.incr + intercept)
px$m2.dsig.decr = transform_y(px$m2.dsig.decr + intercept)
# Plot the slopes alongside the raw data
gamPlot <- ggplot(px, aes(x=curve.x,y=curve.y))+
geom_violin(data=numbers,
draw_quantiles = c(0.05, 0.5, 0.95),
fill = "grey80",scale="area",
aes(x=number_numeric,
y=local_alignment,
group=number_numeric))+
geom_ribbon(
aes(ymin=curve.y.lower,
ymax=curve.y.upper), alpha=0.3)+
geom_line(size=0.5,linetype=3) +
geom_line(aes(x=curve.x,y=m2.dsig.incr),
colour = "blue", size = 2) +
geom_line(aes(x=curve.x,y=m2.dsig.decr),
colour = "red", size = 2) +
xlab("Number")+
ylab("Local alignment") +
scale_x_continuous(trans = "log10",
breaks = c(1,2,3,4,5,6,7,8,10,
12,20,30,40,50,70,100,1000)) +
myThemeBasic+
xlab("Number")+ylab("Semantic Alignment")
gamPlot
```
This shows the difference for 1, a dip for 7 (which the analyses above suggest is due to Uralic languages) and a decrease for 1000.
We now fit a full model with many other controls:
-  Number typology
-  Interaction between numeric value and typology
-  Historical distance (assuming Uralic is maximum distance)
-  Whether the comparison is with a Uralic language
-  Whether the number is 7
-  Interaction between Uralic and 7
-  Whether the number is a Danish irregular
-  Whether the numbers word has a frequent homophone
-  The frequency difference between the forms
```{r}
m1 = bam(local_alignment.center~
s(number_numeric.log.scaled, by=differentNumeralTypology) +
s(hist.dist2) +
s(lang_pair,bs='re') +
isUralic*seven + danish_irregular +
differentNumeralTypology +
homophone +
s(hist.dist2) +
s(freqDiff),
data = numbers)
lrtest(m0,m1)
round(summary(m1)$dev.expl*100,2)
round(summary(m0)$dev.expl*100,2)
summary(m1)
makeFancyGam = function(modelToPlot){
plot.new()
px = plotGAMSignificantSlopes(
modelToPlot,
'number_numeric.log.scaled',
'Number',termPlot=1, interactionTermValue = F)
pxDT = plotGAMSignificantSlopes(
modelToPlot,
'number_numeric.log.scaled',
'Number',termPlot=2,subPlot = 2,subPlotB=1, interactionTermValue = F,
interactionTermValueByVar= list(differentNumeralTypology=T))
transformScale = function(px){
# Model is in scaled space, so transform back:
# Undo scale
px$curve.x = px$curve.x *
attr(numbers$number_numeric.log.scaled,"scaled:scale") +
attr(numbers$number_numeric.log.scaled,"scaled:center")
# Undo log
px$curve.x = exp(px$curve.x)
# Function to do the same for y
transform_y = function(y){
y + attr(numbers$local_alignment.center,"scaled:center")
}
intercept = coef(modelToPlot)[1]
px$curve.y = transform_y(px$curve.y + intercept)
px$curve.y.upper = transform_y(px$curve.y.upper+ intercept)
px$curve.y.lower = transform_y(px$curve.y.lower + intercept)
px$m2.dsig.incr = transform_y(px$m2.dsig.incr + intercept)
px$m2.dsig.decr = transform_y(px$m2.dsig.decr + intercept)
return(px)
}
px = transformScale(px)
pxDT = transformScale(pxDT)
# Plot the slopes alongside the raw data
px$type = "Same numeral typology"
pxDT$type = "Different numeral typology"
pxAll = rbind(px,pxDT)
pxAll$type = factor(pxAll$type)
gamPlot <- ggplot(pxAll, aes(x=curve.x,y=curve.y,group=type))+
geom_violin(data=numbers[
numbers$danish_irregular=="FALSE" &
!((numbers$l1=="hu"| numbers$l2=="hu") & numbers$seven=="TRUE") &
!(numbers$homophone=="TRUE"),],
draw_quantiles = c(0.05, 0.5, 0.95),
fill = "grey80",scale="area",
aes(x=number_numeric,
y=local_alignment,
group=number_numeric))+
# 2nd curve
geom_ribbon(
aes(ymin=curve.y.lower,
ymax=curve.y.upper,
fill=type), alpha=0.2)+
geom_line(aes(x=curve.x,y=curve.y,colour=type),size=0.5,linetype=3) +
geom_line(aes(x=curve.x,y=m2.dsig.incr,colour=type), size = 2) +
geom_line(aes(x=curve.x,y=m2.dsig.decr,colour=type), size = 2) +
# Danish irregulars
geom_point(data=numbers[numbers$danish_irregular=="TRUE",],
aes(x=number_numeric,y=local_alignment,group=NULL),
colour='blue',alpha=0.5) +
# Uralic 7s
geom_point(data=numbers[(numbers$l1=="hu"| numbers$l2=="hu") & numbers$seven=="TRUE",],
aes(x=number_numeric,y=local_alignment,group=NULL),
colour='red',alpha=0.5) +
# Homophones
geom_point(data=numbers[numbers$homophone=="TRUE" &
(numbers$l1=="fr"| numbers$l2=="fr"),],
aes(x=number_numeric,y=local_alignment,group=NULL),
colour='green',alpha=0.5) +
# looks
xlab("Number")+
ylab("Local alignment") +
scale_x_continuous(trans = "log10",
breaks = c(1,2,3,4,5,6,7,8,10,
12,20,30,40,50,70,100,1000)) +
scale_color_manual(values=c("#e66101",'#5e3c99'),name=element_blank())+
scale_fill_manual(values=c("#fdb863", "#b2abd2"), name="fill") +
myThemeBasic+ guides(fill=F) + theme(legend.position = "top") +
xlab("Number")+ylab("Semantic Alignment") +
annotate("text",x=6,y=0.4,label="Hungarian 7",colour="red")+
annotate("text",x=70,y=0.4,label="Danish irregulars",colour="blue") +
annotate("text",x=14,y=0.2,label="Homophones",colour="green")
return(gamPlot)
}
gamPlot = makeFancyGam(m1)
gamPlot
m1.phylo = bam(local_alignment.center~
s(number_numeric.log.scaled, by=differentNumeralTypology) +
s(hist.dist2) +
seven + danish_irregular +
homophone +
differentNumeralTypology +
s(freqDiff),
data = numbers[!is.na(numbers$hist.dist),])
# Model with numeric
summary(m1.phylo)
table(numbers$number_numeric,numbers$sameNumeralTypology)
---
title: "Cognitive influences in language evolution: English data"
output:
pdf_document:
toc: true
---
# Introduction
This is the model code for Monaghan & Roberts, "Cognitive influences in language evolution: Psycholinguistic predictors of loan word borrowing".  It takes data from the WOLD database of borrowing for English and tries to predict whether a word has been borrowed or not according to various psycholinguitic measures.
The main fields in the data frame are:
-  word: Orthographic form
-  borrowing: variable from WOLD indicating level of evidence for borrowing:
-  1 = definately borrowed
-  5 = no evidence of borrowing
-  bor15:  Conversion of the WOLD borrowing variable into a numeric (0 = not borrowed, 1 = borrowed)
-  phonology:  Phonological form
-  phonlength:  Number of segments in the phonological form
-  AoA: Age of acquisition ratings from Kuperman, Stadthagen-Gonzalez, and Brysbaert (2012).
-  AoA\_obj: Objective, test-based age of acuqisition from Brysbaert & Biemiller (2017)
-  subtlexzipf:  Log frequency of word from the SUBTLEX database
-  conc:  Concreteness ratings from Brysbaert, Warriner, & Kuperman (2014)
-  cat: Dominant part of speech according to SUBTLEX.
-  age\_oldest, age\_youngest: Dates from WOLD indicating estiamte of data of entry into English.
-  age\_oldest\_num, age\_youngest\_num, age: Conversions into numeric year values for oldest, youngest and average estimate.
-  source.language: Source language according to WOLD.
-  source: Source language, made more specific by PM.
-  source.word: the source word that was borrowed
-  source.language.mean.word.length: Mean word length of Swadesh list words in the source language (from ASJP database).
-  source.language.word.freq: Frequency of words as long as the source word in the source language (estimated from ASJP)
-  effect: Type of transition (insertion, coexistence, replacement)
-  modern.english, middle.english, old.english: form of the word in various stages of English
-  old.english.length: length of the word in old English
## A note on EDF values and random effects
The Estimated Degrees of Freedom (EDF) is an indication of how non-linear a smooth term is (higher = less linear). It is intended as a diagnostic measure of the shape of the curve, rather than a value used in estimating significance. The EDF is not the same as a simple polynomial curve‚Äôs degree. Instead of a single polynomial curve, each smooth term is a collection of underlying basis functions (simpler curves). When each basis function is weighted by a coefficient, they add up to fit the data. The model attempts to find a collection of basis functions and weighting coefficients that add up to fit the data. Smaller collections of basis functions (simpler models) are preferred and larger (more complex) collections are penalised.
When the model converges on a solution, each smooth term is a collection of simpler curves. Each curve might have a polynomial degree, but it is also useful to have an estimate of the linearity/non-linearity of the whole smooth term.  This is what the EDF provides: an EDF of 1 indicates a linear relationship, and higher values indicate more non-linear relationship. The definition of EDF in the implementaion we use is described in Wood (2008):
> "Associated with each smooth function is one or more measures of function 'wiggliness' $\beta_T^j \tilde{S}_j \beta_j$ where $\tilde{S}_j$
is a matrix of known coefficients. Typically the wiggliness measure evaluates something like the univariate spline penalty."
That is, an EDF value is a combination of non-linearity measures of the basis functions, weighted by the weighting coefficient of each basis function. So, in general, a curve with an EDF of around 2 will look like a quadratic curve, and an EDF of around 3 will look like a cubic curve. However, this does not have to be the case: a smooth term could have a strong linear term, and a very weak non-linear term. The EDF captures this possibility as a continuous value. The simplest way to actually assess the smooth term is to plot it.
Random effects in the GAM implementation we use are treated just like a smooth term with the identity matrix as the penalty coefficient matrix. When entering part of speech as a random (intercept) effect, coefficients are created for each part of speech, modelled as independent and identically distributed normal random variables. The values are defined as discrete points along a smooth function. So, just like in a mixed effects model, the probability of borrowing can be adjusted by a random intercept (the coefficients), e.g. the model can represent nouns as having a higher probability of borrowing, adjectives as slightly less probable and so on. Stronger differences between levels of the random effect would need be represented by more complex functions, which would be penalised (similar to how a linear mixed effect model penalises random effect coefficient estimates which deviate from a normal distribution). The EDF value for the random effects relates to the ‚Äòwiggliness‚Äô of these coefficients when plotted in a regular space. This makes the EDF difficult to interpret.  A random effect where there were no differences between levels would have an EDF of 1 (a flat line), but it would also be 1 when there were consistent distances between each level. So a high EDF would indicate something like an imbalance in the distribution of coefficients. i.e. a few parts of speech are very likely to be borrowed, and most are very unlikely. This is in fact what we have, as shown in table 2 of the manuscript, and is consistent with previous studies of the effect of borrowing relating to grammatical category.
\newpage
```{r echo=F,eval=F}
setwd("~/Documents/MPI/MonaghanAoA/Stats 2/analysis/")
```
# Load libraries
```{r warning=F, message=F}
library(mgcv)
library(sjPlot)
library(lattice)
library(ggplot2)
library(dplyr)
library(party)
library(lmtest)
library(gridExtra)
library(scales)
library(itsadug)
library(ggfortify)
library(factoextra)
library(gridExtra)
library(reshape2)
library(binom)
logit2per = function(X){
return(exp(X)/(1+exp(X)))
}
rescaleGam = function(px, n, xvar, xlab="",breaks=NULL,xlim=NULL){
y = logit2per(px[[n]]$fit)
x = px[[n]]$x *attr(xvar,"scaled:scale") + attr(xvar,"scaled:center")
se.upper = logit2per(px[[n]]$fit+px[[n]]$se)
se.lower = logit2per(px[[n]]$fit-px[[n]]$se)
dx = data.frame(x=x,y=y,ci.upper=se.upper,ci.lower=se.lower)
plen = ggplot(dx, aes(x=x,y=y))+
geom_ribbon(aes(ymin=ci.lower,ymax=ci.upper), alpha=0.3)+
geom_line(size=0.5,linetype=3) +
xlab(xlab)+
ylab("Probability of borrowing")
if(!is.null(breaks)){
plen = plen + scale_x_continuous(breaks = breaks)
}
if(!is.null(xlim)){
plen = plen + coord_cartesian(ylim = c(0,1),xlim=xlim)
} else{
plen = plen + coord_cartesian(ylim = c(0,1))
}
return(plen)
}
# Code for assessing significance of GAM slopes
source("GAM_derivaties.R")
```
# Load data
```{r}
dataloan <- read.csv("../data/loanword12.csv",stringsAsFactors = F)
dataloan$bor15 <- ifelse(dataloan$borrowing==1,1, ifelse(dataloan$borrowing==5,0,NA))
dataloan$bor15.cat <- factor(dataloan$bor15)
```
Convert to numbers.
```{r warning=F}
dataloan$subtlexzipf = as.numeric(dataloan$subtlexzipf)
dataloan$AoA = as.numeric(dataloan$AoA)
dataloan$conc = as.numeric(dataloan$conc)
aoaSD = sd(dataloan$AoA,na.rm = T)
aoaMean = mean(dataloan$AoA/aoaSD,na.rm=T)
dataloan$cat = factor(dataloan$cat)
```
Select only complete cases.
```{r}
dataloan2 = dataloan[complete.cases(dataloan[,
c("phonlength","AoA",
"subtlexzipf", "cat",
'conc','bor15')]),]
```
Scale and center:
```{r}
dataloan2$AoAscale <- scale(dataloan2$AoA)
dataloan2$subtlexzipfscale <- scale(dataloan2$subtlexzipf)
phonlength.center = median(dataloan2$phonlength)
dataloan2$phonlengthscale <-
dataloan2$phonlength - phonlength.center
phonlength.scale = sd(dataloan2$phonlengthscale)
dataloan2$phonlengthscale = dataloan2$phonlengthscale/phonlength.scale
attr(dataloan2$phonlengthscale,"scaled:scale") = phonlength.scale
attr(dataloan2$phonlengthscale,"scaled:center") = phonlength.center
dataloan2$concscale <- scale(dataloan2$conc)
conc.scale = attr(dataloan2$concscale,"scaled:scale")
conc.center = attr(dataloan2$concscale,"scaled:center")
dataloan2$cat = relevel(dataloan2$cat,"Noun")
dataloan2$AoA_objscaled = scale(dataloan2$AoA_obj)
dataloan2$source.language[dataloan2$bor15==0] = "English"
dataloan2$source.language = factor(dataloan2$source.language)
dataloan2$SLMWL = scale(log(dataloan2$source.language.mean.word.length))
dataloan2$SWF = scale(dataloan2$source.language.word.freq)
head(dataloan2$word)
kin= c("mother",'father',"sister","brother")
tapply(dataloan2$bor15,dataloan2$word %in% kin)
tapply(dataloan2$bor15,dataloan2$word %in% kin,mean)
try(setwd("~/Documents/Bristol/word2vec/word2vec_DPLACE/analysis/numbers/"))
